{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaPradhan1/30-seconds-of-cpp/blob/master/DGM_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWzZ4QIR2mjN"
      },
      "source": [
        "# [DGM Assignment 3](https://indianinstituteofscience.sharepoint.com/sites/E1286oDeepGenerativeModelsAugust2024Term/Class%20Files/Forms/AllItems.aspx?FolderCTID=0x012000D9D6326CD0B5934AA2396ACF58032A49&id=%2Fsites%2FE1286oDeepGenerativeModelsAugust2024Term%2FClass%20Files%2FAssignments%2FAssignment%203%2FDGM%5F24%5FA3%2Epdf&parent=%2Fsites%2FE1286oDeepGenerativeModelsAugust2024Term%2FClass%20Files%2FAssignments%2FAssignment%203)\n",
        "\n",
        "> Submitted By:\n",
        "> * Aditya Pradhan\n",
        "> * Aishwarya Bahirat\n",
        "> * Brijgopal Bharadwaj\n",
        "> * Karthikkumar Valoor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOKOqrRX44yO"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "\n",
        "# Common Setup\n",
        "\n",
        "In this section, we generate the artefacts/configurations that are common to multiple sections of the playbook. It's supposed to provide the basic building blocks that can be levaraged across the different stages of our ETL pipelines, training setups, etc. for a centralised management structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54wqBpqoNihD"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsDVeQDX8srw",
        "outputId": "a9cf6a79-e155-4c87-b21a-7cb3b6d64884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DGM_Assignment_2' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone 'https://github.com/BrijAtIISc/DGM_Assignment_2.git'\n",
        "!'/content/DGM_Assignment_2/large_file_manager.sh' recombine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kic1dvPJ76wu",
        "outputId": "513f9786-f3aa-4613-ea92-7745f35ef6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  Testing_set.csv  train  Training_set.csv\n",
            "------------\n",
            "antelope     cockroach\telephant     hippopotamus  lobster    parrot\t      seahorse\tturtle\n",
            "badger\t     cow\tflamingo     hornbill\t   mosquito   pelecaniformes  seal\twhale\n",
            "bat\t     coyote\tfly\t     horse\t   moth       penguin\t      shark\twolf\n",
            "bear\t     crab\tfox\t     hummingbird   mouse      pig\t      sheep\twombat\n",
            "bee\t     crow\tgoat\t     hyena\t   octopus    pigeon\t      snake\twoodpecker\n",
            "beetle\t     deer\tgoldfish     jellyfish\t   okapi      porcupine       sparrow\tzebra\n",
            "bison\t     dog\tgoose\t     kangaroo\t   orangutan  possum\t      squid\n",
            "boar\t     dolphin\tgorilla      koala\t   otter      raccoon\t      squirrel\n",
            "butterfly    donkey\tgrasshopper  ladybugs\t   owl\t      rat\t      starfish\n",
            "cat\t     dragonfly\thamster      leopard\t   ox\t      reindeer\t      swan\n",
            "caterpillar  duck\thare\t     lion\t   oyster     rhinoceros      tiger\n",
            "chimpanzee   eagle\thedgehog     lizard\t   panda      sandpiper       turkey\n"
          ]
        }
      ],
      "source": [
        "# Ensure datasets are accessible\n",
        "!ls '/content/DGM_Assignment_2/datasets/butterflies'\n",
        "!echo '------------'\n",
        "!ls '/content/DGM_Assignment_2/datasets/animals'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NlmCmtfDYjq8"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b5L9xa0s56ss"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import threading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "# Torchvision\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.models import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "import gzip\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kc1Y_0b2hxz",
        "outputId": "d60c8d16-9084-43e3-fffd-9496860fece9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# check if torch finds cuda\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8VT4GEizS8Xx"
      },
      "outputs": [],
      "source": [
        "# PLAYBOOK CONSTANTS\n",
        "ROOT_PATH = f'/content'\n",
        "REPO_PATH = f'{ROOT_PATH}/DGM_Assignment_2'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TRAIN_MODELS = False\n",
        "IMG_DIM = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfFR4cxN_gLK"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Dataset Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvCQZqSX_gLT"
      },
      "source": [
        "### Dataset Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nTKow9Ch_gLT"
      },
      "outputs": [],
      "source": [
        "class ProjectDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data = ImageFolder(data_dir, transform=transform)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    @property\n",
        "    def classes(self):\n",
        "        return self.data.classes\n",
        "\n",
        "    @property\n",
        "    def classmap(self):\n",
        "        return self.data.class_to_idx\n",
        "\n",
        "    @property\n",
        "    def labelmap(self):\n",
        "        return {v: k for k, v in self.data.class_to_idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSmMehDz_gLU"
      },
      "source": [
        "### ETL Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A98u1J_f_gLU"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_DIM, IMG_DIM)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "animals_dataset = ProjectDataset(f'{REPO_PATH}/datasets/animals', transform)\n",
        "butterflies_dataset = ProjectDataset(f'{REPO_PATH}/datasets/butterflies', transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxUrpqJHAIlh"
      },
      "source": [
        "\n",
        "---\n",
        "---\n",
        "\n",
        "# Answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4_HIUnbGt91"
      },
      "source": [
        "---\n",
        "## Problem 1\n",
        "**Train a DDPM on the butterfly dataset. Plot the U-Net training loss curves.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg9dAtUefCF-"
      },
      "source": [
        "##Solution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionForwardProcess:\n",
        "\n",
        "    r\"\"\"\n",
        "\n",
        "    Forward Process class as described in the\n",
        "    paper \"Denoising Diffusion Probabilistic Models\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_time_steps = 1000,\n",
        "                 beta_start = 1e-4,\n",
        "                 beta_end = 0.02\n",
        "                ):\n",
        "\n",
        "        # Precomputing beta, alpha, and alpha_bar for all t's.\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_time_steps)\n",
        "        self.alphas = 1 - self.betas\n",
        "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
        "        self.sqrt_alpha_bars = torch.sqrt(self.alpha_bars)\n",
        "        self.sqrt_one_minus_alpha_bars = torch.sqrt(1 - self.alpha_bars)\n",
        "\n",
        "    def add_noise(self, original, noise, t):\n",
        "\n",
        "        r\"\"\" Adds noise to a batch of original images at time-step t.\n",
        "\n",
        "        :param original: Input Image Tensor\n",
        "        :param noise: Random Noise Tensor sampled from Normal Dist N(0, 1)\n",
        "        :param t: timestep of the forward process of shape -> (B, )\n",
        "\n",
        "        Note: time-step t may differ for each image inside the batch.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        sqrt_alpha_bar_t = self.sqrt_alpha_bars.to(original.device)[t]\n",
        "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_bars.to(original.device)[t]\n",
        "\n",
        "        # Broadcast to multiply with the original image.\n",
        "        sqrt_alpha_bar_t = sqrt_alpha_bar_t[:, None, None, None]\n",
        "        sqrt_one_minus_alpha_bar_t = sqrt_one_minus_alpha_bar_t[:, None, None, None]\n",
        "\n",
        "        # Return\n",
        "        return (sqrt_alpha_bar_t * original) \\\n",
        "                           + \\\n",
        "               (sqrt_one_minus_alpha_bar_t * noise)"
      ],
      "metadata": {
        "id": "TQrS6uBz41b0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "original = torch.randn(4, 1, 28, 28)\n",
        "noise = torch.randn(4, 1, 28, 28)\n",
        "t_steps = torch.randint(0, 1000, (4,))\n",
        "\n",
        "# Forward Process\n",
        "dfp = DiffusionForwardProcess()\n",
        "out = dfp.add_noise(original, noise, t_steps)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhyqfXbI43Y0",
        "outputId": "a450dfa1-2592-43f9-cd65-12c75f79b1c5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionReverseProcess:\n",
        "\n",
        "    r\"\"\"\n",
        "\n",
        "    Reverse Process class as described in the\n",
        "    paper \"Denoising Diffusion Probabilistic Models\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_time_steps = 1000,\n",
        "                 beta_start = 1e-4,\n",
        "                 beta_end = 0.02\n",
        "                ):\n",
        "\n",
        "        # Precomputing beta, alpha, and alpha_bar for all t's.\n",
        "        self.b = torch.linspace(beta_start, beta_end, num_time_steps) # b -> beta\n",
        "        self.a = 1 - self.b # a -> alpha\n",
        "        self.a_bar = torch.cumprod(self.a, dim=0) # a_bar = alpha_bar\n",
        "\n",
        "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
        "\n",
        "        r\"\"\" Sample x_(t-1) given x_t and noise predicted\n",
        "             by model.\n",
        "\n",
        "             :param xt: Image tensor at timestep t of shape -> B x C x H x W\n",
        "             :param noise_pred: Noise Predicted by model of shape -> B x C x H x W\n",
        "             :param t: Current time step\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Original Image Prediction at timestep t\n",
        "        x0 = xt - (torch.sqrt(1 - self.a_bar.to(xt.device)[t]) * noise_pred)\n",
        "        x0 = x0/torch.sqrt(self.a_bar.to(xt.device)[t])\n",
        "        x0 = torch.clamp(x0, -1., 1.)\n",
        "\n",
        "        # mean of x_(t-1)\n",
        "        mean = (xt - ((1 - self.a.to(xt.device)[t]) * noise_pred)/(torch.sqrt(1 - self.a_bar.to(xt.device)[t])))\n",
        "        mean = mean/(torch.sqrt(self.a.to(xt.device)[t]))\n",
        "\n",
        "        # only return mean\n",
        "        if t == 0:\n",
        "            return mean, x0\n",
        "\n",
        "        else:\n",
        "            variance =  (1 - self.a_bar.to(xt.device)[t-1])/(1 - self.a_bar.to(xt.device)[t])\n",
        "            variance = variance * self.b.to(xt.device)[t]\n",
        "            sigma = variance**0.5\n",
        "            z = torch.randn(xt.shape).to(xt.device)\n",
        "\n",
        "            return mean + sigma * z, x0"
      ],
      "metadata": {
        "id": "fGyQPUBJ4_M0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "original = torch.randn(1, 1, 28, 28)\n",
        "noise_pred = torch.randn(1, 1, 28, 28)\n",
        "t = torch.randint(0, 1000, (1,))\n",
        "\n",
        "# Forward Process\n",
        "drp = DiffusionReverseProcess()\n",
        "out, x0 = drp.sample_prev_timestep(original, noise_pred, t)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMgOTttw5Erz",
        "outputId": "359b54e6-ccb7-42f0-f70f-12b892b29ac2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_embedding(\n",
        "    time_steps: torch.Tensor,\n",
        "    t_emb_dim: int\n",
        ") -> torch.Tensor:\n",
        "\n",
        "    \"\"\"\n",
        "    Transform a scalar time-step into a vector representation of size t_emb_dim.\n",
        "\n",
        "    :param time_steps: 1D tensor of size -> (Batch,)\n",
        "    :param t_emb_dim: Embedding Dimension -> for ex: 128 (scalar value)\n",
        "\n",
        "    :return tensor of size -> (B, t_emb_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    assert t_emb_dim%2 == 0, \"time embedding must be divisible by 2.\"\n",
        "\n",
        "    factor = 2 * torch.arange(start = 0,\n",
        "                              end = t_emb_dim//2,\n",
        "                              dtype=torch.float32,\n",
        "                              device=time_steps.device\n",
        "                             ) / (t_emb_dim)\n",
        "\n",
        "    factor = 10000**factor\n",
        "\n",
        "    t_emb = time_steps[:,None] # B -> (B, 1)\n",
        "    t_emb = t_emb/factor # (B, 1) -> (B, t_emb_dim//2)\n",
        "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=1) # (B , t_emb_dim)\n",
        "\n",
        "    return t_emb"
      ],
      "metadata": {
        "id": "BBZ778xT5LeB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NormActConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Perform GroupNorm, Activation, and Convolution operations.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels:int,\n",
        "                 out_channels:int,\n",
        "                 num_groups:int = 8,\n",
        "                 kernel_size: int = 3,\n",
        "                 norm:bool = True,\n",
        "                 act:bool = True\n",
        "                ):\n",
        "        super(NormActConv, self).__init__()\n",
        "\n",
        "        # GroupNorm\n",
        "        self.g_norm = nn.GroupNorm(\n",
        "            num_groups,\n",
        "            in_channels\n",
        "        ) if norm is True else nn.Identity()\n",
        "\n",
        "        # Activation\n",
        "        self.act = nn.SiLU() if act is True else nn.Identity()\n",
        "\n",
        "        # Convolution\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            padding=(kernel_size - 1)//2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.g_norm(x)\n",
        "        x = self.act(x)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps the Time Embedding to the Required output Dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 n_out:int, # Output Dimension\n",
        "                 t_emb_dim:int = 128 # Time Embedding Dimension\n",
        "                ):\n",
        "        super(TimeEmbedding, self).__init__()\n",
        "\n",
        "        # Time Embedding Block\n",
        "        self.te_block = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(t_emb_dim, n_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.te_block(x)\n",
        "\n",
        "\n",
        "\n",
        "class SelfAttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Perform GroupNorm and Multiheaded Self Attention operation.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_channels:int,\n",
        "                 num_groups:int = 8,\n",
        "                 num_heads:int = 4,\n",
        "                 norm:bool = True\n",
        "                ):\n",
        "        super(SelfAttentionBlock, self).__init__()\n",
        "\n",
        "        # GroupNorm\n",
        "        self.g_norm = nn.GroupNorm(\n",
        "            num_groups,\n",
        "            num_channels\n",
        "        ) if norm is True else nn.Identity()\n",
        "\n",
        "        # Self-Attention\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            num_channels,\n",
        "            num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, h, w = x.shape\n",
        "        x = x.reshape(batch_size, channels, h*w)\n",
        "        x = self.g_norm(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x, _ = self.attn(x, x, x)\n",
        "        x = x.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    Perform Downsampling by the factor of k across Height and Width.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels:int,\n",
        "                 out_channels:int,\n",
        "                 k:int = 2, # Downsampling factor\n",
        "                 use_conv:bool = True, # If Downsampling using conv-block\n",
        "                 use_mpool:bool = True # If Downsampling using max-pool\n",
        "                ):\n",
        "        super(Downsample, self).__init__()\n",
        "\n",
        "        self.use_conv = use_conv\n",
        "        self.use_mpool = use_mpool\n",
        "\n",
        "        # Downsampling using Convolution\n",
        "        self.cv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=1),\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels//2 if use_mpool else out_channels,\n",
        "                kernel_size=4,\n",
        "                stride=k,\n",
        "                padding=1\n",
        "            )\n",
        "        ) if use_conv else nn.Identity()\n",
        "\n",
        "        # Downsampling using Maxpool\n",
        "        self.mpool = nn.Sequential(\n",
        "            nn.MaxPool2d(k, k),\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels//2 if use_conv else out_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0\n",
        "            )\n",
        "        ) if use_mpool else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if not self.use_conv:\n",
        "            return self.mpool(x)\n",
        "\n",
        "        if not self.use_mpool:\n",
        "            return self.cv(x)\n",
        "\n",
        "        return torch.cat([self.cv(x), self.mpool(x)], dim=1)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"\n",
        "    Perform Upsampling by the factor of k across Height and Width\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels:int,\n",
        "                 out_channels:int,\n",
        "                 k:int = 2, # Upsampling factor\n",
        "                 use_conv:bool = True, # Upsampling using conv-block\n",
        "                 use_upsample:bool = True # Upsampling using nn.upsample\n",
        "                ):\n",
        "        super(Upsample, self).__init__()\n",
        "\n",
        "        self.use_conv = use_conv\n",
        "        self.use_upsample = use_upsample\n",
        "\n",
        "        # Upsampling using conv\n",
        "        self.cv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels//2 if use_upsample else out_channels,\n",
        "                kernel_size=4,\n",
        "                stride=k,\n",
        "                padding=1\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                out_channels//2 if use_upsample else out_channels,\n",
        "                out_channels//2 if use_upsample else out_channels,\n",
        "                kernel_size = 1,\n",
        "                stride=1,\n",
        "                padding=0\n",
        "            )\n",
        "        ) if use_conv else nn.Identity()\n",
        "\n",
        "        # Upsamling using nn.Upsample\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(\n",
        "                scale_factor=k,\n",
        "                mode = 'bilinear',\n",
        "                align_corners=False\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels//2 if use_conv else out_channels,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0\n",
        "            )\n",
        "        ) if use_upsample else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        if not self.use_conv:\n",
        "            return self.up(x)\n",
        "\n",
        "        if not self.use_upsample:\n",
        "            return self.cv(x)\n",
        "\n",
        "        return torch.cat([self.cv(x), self.up(x)], dim=1)"
      ],
      "metadata": {
        "id": "_DvrB3V-5OLM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = Upsample(16, 32, 2, True, True)\n",
        "x = torch.randn(4, 16, 32, 32)\n",
        "layer(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKm1QTO25XgW",
        "outputId": "533b4147-b4d9-46b8-d156-3ec0619713da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 32, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownC(nn.Module):\n",
        "    \"\"\"\n",
        "    Perform Down-convolution on the input using following approach.\n",
        "    1. Conv + TimeEmbedding\n",
        "    2. Conv\n",
        "    3. Skip-connection from input x.\n",
        "    4. Self-Attention\n",
        "    5. Skip-Connection from 3.\n",
        "    6. Downsampling\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels:int,\n",
        "                 out_channels:int,\n",
        "                 t_emb_dim:int = 128, # Time Embedding Dimension\n",
        "                 num_layers:int=1,\n",
        "                 down_sample:bool = True # True for Downsampling\n",
        "                ):\n",
        "        super(DownC, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.conv1 = nn.ModuleList([\n",
        "            NormActConv(in_channels if i==0 else out_channels,\n",
        "                        out_channels\n",
        "                       ) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.conv2 = nn.ModuleList([\n",
        "            NormActConv(out_channels,\n",
        "                        out_channels\n",
        "                       ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.te_block = nn.ModuleList([\n",
        "            TimeEmbedding(out_channels, t_emb_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.attn_block = nn.ModuleList([\n",
        "            SelfAttentionBlock(out_channels) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.down_block =Downsample(out_channels, out_channels) if down_sample else nn.Identity()\n",
        "\n",
        "        self.res_block = nn.ModuleList([\n",
        "            nn.Conv2d(\n",
        "                in_channels if i==0 else out_channels,\n",
        "                out_channels,\n",
        "                kernel_size=1\n",
        "            ) for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "\n",
        "        out = x\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "\n",
        "            # Resnet Block\n",
        "            out = self.conv1[i](out)\n",
        "            out = out + self.te_block[i](t_emb)[:, :, None, None]\n",
        "            out = self.conv2[i](out)\n",
        "            out = out + self.res_block[i](resnet_input)\n",
        "\n",
        "            # Self Attention\n",
        "            out_attn = self.attn_block[i](out)\n",
        "            out = out + out_attn\n",
        "\n",
        "        # Downsampling\n",
        "        out = self.down_block(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "sdRwvx2b5jrB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MidC(nn.Module):\n",
        "    \"\"\"\n",
        "    Refine the features obtained from the DownC block.\n",
        "    It refines the features using following operations:\n",
        "\n",
        "    1. Resnet Block with Time Embedding\n",
        "    2. A Series of Self-Attention + Resnet Block with Time-Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels:int,\n",
        "                 out_channels:int,\n",
        "                 t_emb_dim:int = 128,\n",
        "                 num_layers:int = 1\n",
        "                ):\n",
        "        super(MidC, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.conv1 = nn.ModuleList([\n",
        "            NormActConv(in_channels if i==0 else out_channels,\n",
        "                        out_channels\n",
        "                       ) for i in range(num_layers + 1)\n",
        "        ])\n",
        "\n",
        "        self.conv2 = nn.ModuleList([\n",
        "            NormActConv(out_channels,\n",
        "                        out_channels\n",
        "                       ) for _ in range(num_layers + 1)\n",
        "        ])\n",
        "\n",
        "        self.te_block = nn.ModuleList([\n",
        "            TimeEmbedding(out_channels, t_emb_dim) for _ in range(num_layers + 1)\n",
        "        ])\n",
        "\n",
        "        self.attn_block = nn.ModuleList([\n",
        "            SelfAttentionBlock(out_channels) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.res_block = nn.ModuleList([\n",
        "            nn.Conv2d(\n",
        "                in_channels if i==0 else out_channels,\n",
        "                out_channels,\n",
        "                kernel_size=1\n",
        "            ) for i in range(num_layers + 1)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        out = x\n",
        "\n",
        "        # First-Resnet Block\n",
        "        resnet_input = out\n",
        "        out = self.conv1[0](out)\n",
        "        out = out + self.te_block[0](t_emb)[:, :, None, None]\n",
        "        out = self.conv2[0](out)\n",
        "        out = out + self.res_block[0](resnet_input)\n",
        "\n",
        "        # Sequence of Self-Attention + Resnet Blocks\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            # Self Attention\n",
        "            out_attn = self.attn_block[i](out)\n",
        "            out = out + out_attn\n",
        "\n",
        "            # Resnet Block\n",
        "            resnet_input = out\n",
        "            out = self.conv1[i+1](out)\n",
        "            out = out + self.te_block[i+1](t_emb)[:, :, None, None]\n",
        "            out = self.conv2[i+1](out)\n",
        "            out = out + self.res_block[i+1](resnet_input)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "-Go0Fa1P5kof"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpC(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, t_emb_dim: int = 128, num_layers: int = 1, up_sample: bool = True):\n",
        "        super(UpC, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.conv1 = nn.ModuleList([\n",
        "            NormActConv(in_channels if i == 0 else out_channels, out_channels)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        self.conv2 = nn.ModuleList([\n",
        "            NormActConv(out_channels, out_channels)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.te_block = nn.ModuleList([\n",
        "            TimeEmbedding(out_channels, t_emb_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.attn_block = nn.ModuleList([\n",
        "            SelfAttentionBlock(out_channels)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.up_block = Upsample(in_channels, in_channels // 2) if up_sample else nn.Identity()\n",
        "\n",
        "        # Add a convolutional layer to match dimensions\n",
        "        self.align_channels = nn.Conv2d(in_channels // 2, out_channels, kernel_size=1)\n",
        "\n",
        "        self.res_block = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, down_out, t_emb):\n",
        "        # Upsampling\n",
        "        x = self.up_block(x)\n",
        "\n",
        "        # Align channels of x with down_out\n",
        "        x = self.align_channels(x)\n",
        "\n",
        "        # Concatenate along channel dimension\n",
        "        x = torch.cat([x, down_out], dim=1)\n",
        "\n",
        "        out = x\n",
        "        for i in range(self.num_layers):\n",
        "            resnet_input = out\n",
        "            out = self.conv1[i](out)\n",
        "            out = out + self.te_block[i](t_emb)[:, :, None, None]\n",
        "            out = self.conv2[i](out)\n",
        "            out = out + self.res_block[i](resnet_input)\n",
        "\n",
        "            # Self Attention\n",
        "            out_attn = self.attn_block[i](out)\n",
        "            out = out + out_attn\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "yDioHVRu5xdV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-net architecture which is used to predict noise\n",
        "    in the paper \"Denoising Diffusion Probabilistic Model\".\n",
        "\n",
        "    U-net consists of Series of DownC blocks followed by MidC\n",
        "    followed by UpC.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 im_channels: int = 3,\n",
        "                 down_ch: list = [16, 32, 64],\n",
        "                 mid_ch: list = [64, 64],\n",
        "                 up_ch: list[int] = [64, 32, 16],\n",
        "                 down_sample: list[bool] = [True, True, False],\n",
        "                 t_emb_dim: int = 128,\n",
        "                 num_downc_layers:int = 1,\n",
        "                 num_midc_layers:int = 1,\n",
        "                 num_upc_layers:int = 1\n",
        "                ):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.im_channels = im_channels\n",
        "        self.down_ch = down_ch\n",
        "        self.mid_ch = mid_ch\n",
        "        self.up_ch = up_ch\n",
        "        self.t_emb_dim = t_emb_dim\n",
        "        self.down_sample = down_sample\n",
        "        self.num_downc_layers = num_downc_layers\n",
        "        self.num_midc_layers = num_midc_layers\n",
        "        self.num_upc_layers = num_upc_layers\n",
        "\n",
        "        self.up_sample = list(reversed(self.down_sample)) # [False, True, True]\n",
        "\n",
        "        # Initial Convolution\n",
        "        self.cv1 = nn.Conv2d(self.im_channels, self.down_ch[0], kernel_size=3, padding=1)\n",
        "\n",
        "        # Initial Time Embedding Projection\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "        )\n",
        "\n",
        "        # DownC Blocks\n",
        "        self.downs = nn.ModuleList([\n",
        "            DownC(\n",
        "                self.down_ch[i],\n",
        "                self.down_ch[i+1],\n",
        "                self.t_emb_dim,\n",
        "                self.num_downc_layers,\n",
        "                self.down_sample[i]\n",
        "            ) for i in range(len(self.down_ch) - 1)\n",
        "        ])\n",
        "\n",
        "        # MidC Block\n",
        "        self.mids = nn.ModuleList([\n",
        "            MidC(\n",
        "                self.mid_ch[i],\n",
        "                self.mid_ch[i+1],\n",
        "                self.t_emb_dim,\n",
        "                self.num_midc_layers\n",
        "            ) for i in range(len(self.mid_ch) - 1)\n",
        "        ])\n",
        "\n",
        "        # UpC Block\n",
        "        self.ups = nn.ModuleList([\n",
        "            UpC(\n",
        "                self.up_ch[i],\n",
        "                self.up_ch[i+1],\n",
        "                self.t_emb_dim,\n",
        "                self.num_upc_layers,\n",
        "                self.up_sample[i]\n",
        "            ) for i in range(len(self.up_ch) - 1)\n",
        "        ])\n",
        "\n",
        "        # Final Convolution\n",
        "        self.cv2 = nn.Sequential(\n",
        "            nn.GroupNorm(8, self.up_ch[-1]),\n",
        "            nn.Conv2d(self.up_ch[-1], self.im_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "\n",
        "        out = self.cv1(x)\n",
        "\n",
        "        # Time Projection\n",
        "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
        "        t_emb = self.t_proj(t_emb)\n",
        "\n",
        "        # DownC outputs\n",
        "        down_outs = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t_emb)\n",
        "\n",
        "        # MidC outputs\n",
        "        for mid in self.mids:\n",
        "            out = mid(out, t_emb)\n",
        "\n",
        "        # UpC Blocks\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            out = up(out, down_out, t_emb)\n",
        "\n",
        "        # Final Conv\n",
        "        out = self.cv2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "iXANvzm6-Abf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test with smaller batch size\n",
        "# model = Unet()\n",
        "# x = torch.randn(1, 3, 32, 32) # Reduced batch size to 2\n",
        "# t = torch.randint(0, 10, (1,)) # Adjust t accordingly\n",
        "# model(x, t).shape"
      ],
      "metadata": {
        "id": "AMTUN3_B5xY0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "class CustomMnistDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Reads the MNIST data from csv file given file path.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_path, num_datapoints = None):\n",
        "        super(CustomMnistDataset, self).__init__()\n",
        "\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Will be useful later while evaluating\n",
        "        if num_datapoints is not None:\n",
        "            self.df = self.df.iloc[0:num_datapoints]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def  __getitem__(self, index):\n",
        "        # Read\n",
        "        img = self.df.iloc[index].filter(regex='pixel').values\n",
        "        img =  np.reshape(img, (28, 28)).astype(np.uint8)\n",
        "\n",
        "        # Convert to Tensor\n",
        "        img_tensor = torchvision.transforms.ToTensor()(img) # [0, 1]\n",
        "        img_tensor = 2*img_tensor - 1 # [-1, 1]\n",
        "\n",
        "        return img_tensor"
      ],
      "metadata": {
        "id": "1ELB-8vc5xWj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CONFIG:\n",
        "    model_path = 'ddpm_unet.pth'\n",
        "    num_epochs = 10 #later change to 50\n",
        "    lr = 1e-4\n",
        "    num_timesteps = 1000\n",
        "    batch_size = 1\n",
        "    img_size = 16\n",
        "    in_channels = 1\n",
        "    num_img_to_generate = 100"
      ],
      "metadata": {
        "id": "EDgMEOeA5xUP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from skimage import io\n",
        "class ButterflyDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.annotations = pd.read_csv(f'{root_dir}/Training_set.csv')\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Encode labels and set class mappings\n",
        "        label_encoder = LabelEncoder()\n",
        "        self.annotations['y_label'] = label_encoder.fit_transform(self.annotations['label'])\n",
        "\n",
        "        # Setting the classes, idx_to_class, and class_to_idx properties\n",
        "        self.classes = label_encoder.classes_.tolist()\n",
        "        self.idx_to_class = {idx: class_name for idx, class_name in enumerate(self.classes)}\n",
        "        self.class_to_idx = {class_name: idx for idx, class_name in self.idx_to_class.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(f'{self.root_dir}/train', self.annotations['filename'][index])\n",
        "        img = io.imread(img_path)\n",
        "        y_label = int(self.annotations['y_label'][index])\n",
        "\n",
        "        if self.transform:\n",
        "            img = Image.fromarray(img)\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, y_label"
      ],
      "metadata": {
        "id": "G-KDpXnkCbND"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "# from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train(cfg):\n",
        "\n",
        "    # Dataset and Dataloader\n",
        "    butterflies_dataset = ButterflyDataset(f'{REPO_PATH}/datasets/butterflies', transform)\n",
        "    reduced_dataset = torch.utils.data.Subset(butterflies_dataset, indices=range(0, 500)) # Using only 1000 samples\n",
        "    butterflies_dl = DataLoader(reduced_dataset, cfg.batch_size, shuffle=True,pin_memory=True)\n",
        "\n",
        "    # Device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Device: {device}\\n')\n",
        "\n",
        "    # Initiate Model\n",
        "    model = Unet().to(device)\n",
        "\n",
        "    # Initialize Optimizer and Loss Function\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    # Diffusion Forward Process to add noise\n",
        "    dfp = DiffusionForwardProcess()\n",
        "\n",
        "    # Best Loss\n",
        "    best_eval_loss = float('inf')\n",
        "    # scaler = GradScaler()\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "\n",
        "        # For Loss Tracking\n",
        "        losses = []\n",
        "\n",
        "        # Set model to train mode\n",
        "        model.train()\n",
        "\n",
        "        # Loop over dataloader\n",
        "        for imgs, labels in tqdm(butterflies_dl): # Unpack the tuple here\n",
        "\n",
        "            imgs = imgs.to(device) # Move imgs to device\n",
        "            labels = labels.to(device) # Move labels to device if needed\n",
        "\n",
        "            # Generate noise and timestamps\n",
        "            noise = torch.randn_like(imgs).to(device)\n",
        "            t = torch.randint(0, cfg.num_timesteps, (imgs.shape[0],)).to(device)\n",
        "\n",
        "            # Add noise to the images using Forward Process\n",
        "            noisy_imgs = dfp.add_noise(imgs, noise, t)\n",
        "\n",
        "            # Avoid Gradient Accumulation\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Predict noise using U-net Model\n",
        "            noise_pred = model(noisy_imgs, t)\n",
        "\n",
        "            # Calculate Loss\n",
        "            loss = criterion(noise_pred, noise)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # Backprop + Update model params\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # scaler.scale(loss).backward()\n",
        "            # scaler.step(optimizer)\n",
        "            # scaler.update()\n",
        "\n",
        "        # Mean Loss\n",
        "        mean_epoch_loss = np.mean(losses)\n",
        "\n",
        "        # Display\n",
        "        print('Epoch:{} | Loss : {:.4f}'.format(\n",
        "            epoch + 1,\n",
        "            mean_epoch_loss,\n",
        "        ))\n",
        "\n",
        "        # Save based on train-loss\n",
        "        if mean_epoch_loss < best_eval_loss:\n",
        "            best_eval_loss = mean_epoch_loss\n",
        "            torch.save(model, cfg.model_path)\n",
        "\n",
        "    print(f'Done training.....')"
      ],
      "metadata": {
        "id": "THjdMRY-5xPt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "cfg = CONFIG()"
      ],
      "metadata": {
        "id": "JOgG5mz75xKp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN\n",
        "train(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9G_q0ivd-GW",
        "outputId": "0a75e2de-d55d-49c1-b8f6-a8ee8dad5c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(cfg):\n",
        "    \"\"\"\n",
        "    Given Pretrained DDPM U-net model, Generate Real-life\n",
        "    Images from noise by going backward step by step. i.e.,\n",
        "    Mapping of Random Noise to Real-life images.\n",
        "    \"\"\"\n",
        "\n",
        "    # Device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    #print(f'Device: {device}\\n')\n",
        "\n",
        "    # Initialize Diffusion Reverse Process\n",
        "    drp = DiffusionReverseProcess()\n",
        "\n",
        "    # Set model to eval mode\n",
        "    model = torch.load(cfg.model_path).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Generate Noise sample from N(0, 1)\n",
        "    xt = torch.randn(1, cfg.in_channels, cfg.img_size, cfg.img_size).to(device)\n",
        "\n",
        "    # Denoise step by step by going backward.\n",
        "    with torch.no_grad():\n",
        "        for t in reversed(range(cfg.num_timesteps)):\n",
        "            noise_pred = model(xt, torch.as_tensor(t).unsqueeze(0).to(device))\n",
        "            xt, x0 = drp.sample_prev_timestep(xt, noise_pred, torch.as_tensor(t).to(device))\n",
        "\n",
        "    # Convert the image to proper scale\n",
        "    xt = torch.clamp(xt, -1., 1.).detach().cpu()\n",
        "    xt = (xt + 1) / 2\n",
        "\n",
        "    return xt"
      ],
      "metadata": {
        "id": "a1-f27bd5xIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and config\n",
        "cfg = CONFIG()\n",
        "\n",
        "# Generate\n",
        "generated_imgs = []\n",
        "for i in tqdm(range(cfg.num_img_to_generate)):\n",
        "    xt = generate(cfg)\n",
        "    xt = 255 * xt[0][0].numpy()\n",
        "    generated_imgs.append(xt.astype(np.uint8).flatten())\n",
        "\n",
        "# Save Generated Data CSV\n",
        "generated_df = pd.DataFrame(generated_imgs, columns=[f'pixel{i}' for i in range(784)])\n",
        "generated_df.to_csv(cfg.generated_csv_path, index=False)\n",
        "\n",
        "# Visualize\n",
        "from matplotlib import pyplot as plt\n",
        "fig, axes = plt.subplots(8, 8, figsize=(5, 5))\n",
        "\n",
        "# Plot each image in the corresponding subplot\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(np.reshape(generated_imgs[i], (28, 28)), cmap='gray')  # You might need to adjust the colormap based on your images\n",
        "    ax.axis('off')  # Turn off axis labels\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing between subplots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3nDa3Aw05xCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oTgMViAC5xAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pghkrVgX5w7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlkyJ_m25w4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qCLSVu0E5w1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HdPHzC525wxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pRP5oAq15wsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxhez-eCHUtc"
      },
      "source": [
        "---\n",
        "## Problem 2\n",
        "**Plot a 10 by 10 grid of images for generated images.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHpEP-yRKyGV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Behp8kHXoc"
      },
      "source": [
        "---\n",
        "## Problem 3\n",
        "**Compute FID by sampling 1000 data-points from both the true and the generated data distributions.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MmZuqHRK1wc"
      },
      "outputs": [],
      "source": [
        "def get_activation(dataloader,\n",
        "                   model,\n",
        "                   preprocess, # Preprocessing Transform for InceptionV3\n",
        "                   device = 'cpu'\n",
        "                  ):\n",
        "    \"\"\"\n",
        "    Given Dataloader and Model, Generate N X 2048\n",
        "    Dimensional activation map for N data points\n",
        "    in dataloader.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set model to evaluation Mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Save activations\n",
        "    pred_arr = np.zeros((len(dataloader.dataset), 2048))\n",
        "\n",
        "    # Batch Size\n",
        "    batch_size = dataloader.batch_size\n",
        "\n",
        "    # Loop over Dataloader\n",
        "    with torch.no_grad():\n",
        "        for i, batch in tqdm(enumerate(dataloader)):\n",
        "\n",
        "            # Transform the Batch according to Inceptionv3 specification\n",
        "            batch = torch.stack([preprocess(img) for img in batch]).to(device)\n",
        "\n",
        "            # Predict\n",
        "            pred = model(batch).cpu().numpy()\n",
        "\n",
        "            # Store\n",
        "            pred_arr[i*batch_size : i*batch_size + batch.size(0), :] = pred\n",
        "\n",
        "    return pred_arr\n",
        "\n",
        "#--------------------------------------------------------------------\n",
        "\n",
        "def calculate_activation_statistics(dataloader,\n",
        "                                    model,\n",
        "                                    preprocess,\n",
        "                                    device='cpu'\n",
        "                                   ):\n",
        "    \"\"\"\n",
        "    Get mean vector and covariance matrix of the activation maps.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get activation maps\n",
        "    act = get_activation(dataloader,\n",
        "                         model,\n",
        "                         preprocess, # Preprocessing Transform for InceptionV3\n",
        "                         device\n",
        "                       )\n",
        "    # Mean\n",
        "    mu = np.mean(act, axis=0)\n",
        "\n",
        "    # Covariance Metric\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "\n",
        "    return mu, sigma\n",
        "\n",
        "#----------------------------------------------------------------\n",
        "from scipy import linalg\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "\n",
        "    \"\"\"\n",
        "    Given Mean and Sigma of Real and Generated Data,\n",
        "    it calculates FID between them using:\n",
        "\n",
        "     d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "\n",
        "    \"\"\"\n",
        "    # Make sure they have appropriate dims\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "\n",
        "    # Handle various cases\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = (\n",
        "            \"fid calculation produces singular product; \"\n",
        "            \"adding %s to diagonal of cov estimates\"\n",
        "        ) % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError(\"Imaginary component {}\".format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform to Convert Output of CustomMnistDataset class to Inception format.\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_inception = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: (x + 1.0)/2.0), # [-1, 1] => [0, 1]\n",
        "    transforms.ToPILImage(), # Tensor to PIL Image\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB format\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
        "\n",
        "])\n",
        "\n",
        "# Load InceptionV3 Model\n",
        "import torchvision.models as models\n",
        "from torchvision.models.inception import Inception_V3_Weights\n",
        "model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
        "model.fc = nn.Identity()\n",
        "\n",
        "# Mean and Sigma For Generated Data\n",
        "mnist_ds = CustomMnistDataset(cfg.generated_csv_path, cfg.num_img_to_generate)\n",
        "mnist_dl = DataLoader(mnist_ds, cfg.batch_size//4, shuffle=False)\n",
        "mu1, sigma1 = calculate_activation_statistics(mnist_dl, model, preprocess = transform_inception, device='cuda')\n",
        "\n",
        "# Mean and Sigma for Test Data\n",
        "mnist_ds = CustomMnistDataset(cfg.test_csv_path, cfg.num_img_to_generate)\n",
        "mnist_dl = DataLoader(mnist_ds, cfg.batch_size//4, shuffle=False)\n",
        "mu2, sigma2 = calculate_activation_statistics(mnist_dl, model, preprocess = transform_inception, device='cuda')\n",
        "\n",
        "# Calculate FID\n",
        "fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
        "print(f'FID-Score: {fid}')"
      ],
      "metadata": {
        "id": "FRtowXxG6Yza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c9LuQ8_HZf_"
      },
      "source": [
        "---\n",
        "## Problem 4\n",
        "**Repeat the above experiments by training the DDPM on the latent space of the VQ-VAE trained in the previous assignment. You should compute the FID and plot the images.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7Yd9QF0K2qC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XZDk1YFHb4F"
      },
      "source": [
        "---\n",
        "## Problem 5\n",
        "**Implement conditional generation using classifier-guided diffusion.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdoLy-GelQvS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmHR6Ws1Hdch"
      },
      "source": [
        "---\n",
        "## Problem 6\n",
        "**By using the same network trained for DDPM,implement a DDIM sampler and compare the generation quality (via FID) with the DDPM. Note that this question does not need any additional training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CaI3ut1LVT-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiEslpHCHfLb"
      },
      "source": [
        "---\n",
        "## Problem 7\n",
        "**Implement a DDIM inversion method. Get the (inverted) latents for a pair of real images and plot the generated images obtained via linear interpolation of the latents corresponding to these images.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gZsIvhGLfyS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt4jZPZdHg6a"
      },
      "source": [
        "---\n",
        "## Problem 8\n",
        "**Train a Resnet-50 on the Animal dataset and measure the accuracy on the test dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzsLoxcML-XT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cDkL2J5Hjo3"
      },
      "source": [
        "---\n",
        "## Problem 9\n",
        "**Distill the above resent on a small-sized MLP (using KL distillation loss across logits) and measure the test accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUjKEhdzMQip"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh1IhcGYHlj7"
      },
      "source": [
        "---\n",
        "## Problem 10\n",
        " **Implement i-JEPG on the animal dataset and train the same small-sized used in the distillation question and compare the accuracies.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vk1gojAMvtEX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "mfFR4cxN_gLK",
        "QvCQZqSX_gLT",
        "mSmMehDz_gLU",
        "qmHR6Ws1Hdch"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}